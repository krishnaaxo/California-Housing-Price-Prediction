# -*- coding: utf-8 -*-
"""California Housing Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TWGuCtXllzrlNEb24XXi93NSMt7lkmta

IMPORTING PACKAGES
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
# %matplotlib inline

"""SPLITTING DATA"""

dataset = pd.read_csv('housing.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
y = y.reshape(len(y),1)

"""MEDIAN_HOUSE_VALUE IS THE SPECIAL FEATURE"""

dataset['median_house_value']

"""PLOTTING FREQUENCY VS MEDIAN HOUSE PRICE"""

plt.subplots(figsize=(12,9))
sns.distplot(dataset['median_house_value'], fit=stats.norm)

# Get the fitted parameters used by the function

(mu, sigma) = stats.norm.fit(dataset['median_house_value'])

# plot with the distribution

plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

#Probablity plot

fig = plt.figure()
stats.probplot(dataset['median_house_value'], plot=plt)
plt.show()

"""NORMAL DISTRIBUTION PLOTTING"""

#we use log function which is in numpy
dataset['median_house_value'] = np.log1p(dataset['median_house_value'])

#Check again for more normal distribution

plt.subplots(figsize=(12,9))
sns.distplot(dataset['median_house_value'], fit=stats.norm)

# Get the fitted parameters used by the function

(mu, sigma) = stats.norm.fit(dataset['median_house_value'])

# plot with the distribution

plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')

#Probablity plot

fig = plt.figure()
stats.probplot(dataset['median_house_value'], plot=plt)
plt.show()

"""FIND NULL VALUES IN THE DATASET"""

plt.figure(figsize=(12, 6))
sns.heatmap(dataset.isnull())
plt.show()

dataset.isnull().sum()

"""Corralation between all Attributes"""

train_corr = dataset.select_dtypes(include=[np.number])

corr = train_corr.corr()
plt.subplots(figsize=(20,9))
sns.heatmap(corr, annot=True)

"""MEDIAN INCOME AND TOTAL ROOMS HAVE THE HIGHEST CORRELATION"""

#unique value of median_income
dataset.median_income.unique()

"""BAR PLOT"""

sns.barplot(dataset.median_income, dataset.median_house_value)

"""BOXPLOT"""

plt.figure(figsize=(18, 8))
sns.boxplot(dataset.median_income, dataset.median_house_value)

"""PAIRPLOT"""

col = ['median_house_value', 'median_income', 'longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms']
sns.set(style='ticks')
sns.pairplot(dataset[col], height=3, kind='reg')

"""Histogram of all Attributes"""

dataset.hist(figsize=(25,25),bins=50);

dataset.plot(kind='scatter', x='longitude', y='latitude', alpha=0.9, 
    s=dataset['population']/10, label='population', figsize=(14,10), 
    c='median_house_value', cmap=plt.get_cmap('cool'), colorbar=True)

dataset.plot(kind='scatter', x='longitude', y='latitude', alpha=0.9, 
    s=dataset['population']/100, label='population', figsize=(14,10), 
    c='median_house_value', cmap=plt.get_cmap('prism'), colorbar=True)

"""FILLING ALL THE NULL VALUES"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 4:5])
X[:, 4:5] = imputer.transform(X[:, 4:5])
print(X)

"""ENCODING THE CATEGORICAL DATA"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [8])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)

"""SPLITTING INTO TRAIN AND TEST SET"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""APPLYING STANDARDSCALER"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train = sc_X.fit_transform(X_train)
y_train = sc_y.fit_transform(y_train)

"""1)TRAIN WITH SVM MODEL"""

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X_train, y_train)

"""Predicting the Test set results"""

y_pred = sc_y.inverse_transform(regressor.predict(sc_X.transform(X_test)))
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Evaluating the Model Performance"""

from sklearn.metrics import r2_score
rr1=r2_score(y_test, y_pred)
print(rr1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test= sc.transform(X_test)

"""2) APPLYING RANDOM FOREST REGRESSION MODEL"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 400, random_state = 0)
regressor.fit(X_train, y_train)

"""Predicting the Test set results"""

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Evaluating the Model Performance"""

from sklearn.metrics import r2_score
rr2=r2_score(y_test, y_pred)
print(rr2)

"""RMSE"""

from sklearn.metrics import mean_squared_error
predictions = regressor.predict(X_test)
lin_mse = mean_squared_error(y_test,predictions)
lin_rmse = np.sqrt(lin_mse)
print('rmse value is : ',lin_rmse)

"""3)APPLYING MULTIPLE REGRESSION"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

"""Predicting the Test set results"""

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Evaluating the Model Performance"""

from sklearn.metrics import r2_score
rr3=r2_score(y_test, y_pred)
print(rr3)

"""RMSE"""

from sklearn.metrics import mean_squared_error
predictions = regressor.predict(X_test)
lin_mse = mean_squared_error(y_test,predictions)
lin_rmse = np.sqrt(lin_mse)
print('rmse value is : ',lin_rmse)

"""4)APPLYING XGBOOST MODEL"""

from xgboost import XGBRegressor
regressor = XGBRegressor()
regressor.fit(X_train, y_train)

"""Predicting the Test set results"""

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Evaluating the Model Performance"""

from sklearn.metrics import r2_score
rr4=r2_score(y_test, y_pred)
print(rr4)

"""RMSE"""

from sklearn.metrics import mean_squared_error
predictions = regressor.predict(X_test)
lin_mse = mean_squared_error(y_test,predictions)
lin_rmse = np.sqrt(lin_mse)
print('rmse value is : ',lin_rmse)

"""Overall Model Performance"""

data = [rr1, rr2, rr3, rr4]
index = ['SVM', 'Random Forest Regressor','Multiple Regression','XGBOOST']
pd.DataFrame(data, index=index, columns=['Scores']).sort_values(ascending = False, by=['Scores'])

"""5) APPLYING ANN

IMPORTING PACKAGES
"""

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LeakyReLU,PReLU,ELU
from keras.layers import Dropout

"""Building the ANN"""

clas = Sequential()

"""Adding the input layer and the first hidden layer"""

clas.add(Dense(units= 11, kernel_initializer= 'he_uniform',activation='relu'))

"""Adding the  hidden layer"""

clas.add(Dense(units= 400, kernel_initializer= 'he_uniform',activation='relu'))

"""Adding the  hidden layer"""

clas.add(Dense(units=800, kernel_initializer='he_uniform',activation='relu'))

"""Adding the hidden layer"""

clas.add(Dense(units=800, kernel_initializer='he_uniform',activation='relu'))

"""Adding the hidden layer"""

clas.add(Dense(units=800, kernel_initializer='he_uniform',activation='relu'))

"""Adding the hidden layer"""

clas.add(Dense(units= 800, kernel_initializer= 'he_uniform',activation='relu'))

"""Adding the output layer"""

clas.add(Dense(units = 1))

"""Compiling the ANN"""

clas.compile(optimizer = 'adam', loss = 'mean_squared_error' )

"""Training the ANN model on the Training set"""

model_history=clas.fit(X_train, y_train, batch_size = 60, epochs = 100)

plt.plot(model_history.history['loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Evaluating the Model Performance"""

y_pred = clas.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Evaluating the Model Performance"""

from sklearn.metrics import r2_score
rr5=r2_score(y_test, y_pred)
print(rr5)

data = [rr1, rr2, rr3, rr4,rr5]
index = ['SVM', 'Random Forest Regressoion','Multiple Regression','XGBOOST','ANN']
pd.DataFrame(data, index=index, columns=['Scores']).sort_values(ascending = False, by=['Scores'])

"""The winner is Random Forest Regression . Random Forest Regression model can be used for prediction and The median income is the number one predictor of housing prices.ANN model can be further improved but it will consume more time to train the model."""